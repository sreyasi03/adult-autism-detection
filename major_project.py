# -*- coding: utf-8 -*-
"""Major_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Fhlj8nd4fy2Ny8SBg4bVv2k8C0fL0UR7
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
df=pd.read_csv('/content/autism_screening.csv')
print(df)

df

data1=df.drop(['age','gender','ethnicity','country_of_res','ethnicity','age_desc','relation'],axis=1)

data1

data1=data1.rename(columns={"Class/ASD":"Class"})

plt.figure(figsize=(3,3))
sns.distplot(data1.A1_Score[data1.Class==0])
sns.distplot(data1.A1_Score[data1.Class==1])
plt.legend(['has ASD','no ASD'])

sns.countplot(x=data1.A2_Score,hue=data1.Class)
plt.show()

sns.countplot(x=data1.A3_Score,hue=data1.Class)
plt.show()

sns.countplot(x=data1.A4_Score,hue=data1.Class)
plt.show()

sns.countplot(x=data1.A5_Score,hue=data1.Class)
plt.show()

sns.countplot(x=data1.A6_Score,hue=data1.Class)
plt.show()

sns.countplot(x=data1.A7_Score,hue=data1.Class)
plt.show()

sns.countplot(x=data1.A8_Score,hue=data1.Class)
plt.show()

sns.countplot(x=data1.A9_Score,hue=data1.Class)
plt.show()

sns.countplot(x=data1.A10_Score,hue=data1.Class)
plt.show()

plt.figure(figsize=(3,3))
sns.distplot(data1.A2_Score[data1.Class==0])
sns.distplot(data1.A2_Score[data1.Class==1])
plt.legend(['has ASD','no ASD'])

plt.figure(figsize=(3,3))
sns.distplot(data1.A3_Score[data1.Class==0])
sns.distplot(data1.A3_Score[data1.Class==1])
plt.legend(['has ASD','no ASD'])

plt.figure(figsize=(3,3))
sns.distplot(data1.A4_Score[data1.Class==0])
sns.distplot(data1.A4_Score[data1.Class==1])
plt.legend(['has ASD','no ASD'])

plt.figure(figsize=(3,3))
sns.distplot(data1.A5_Score[data1.Class==0])
sns.distplot(data1.A5_Score[data1.Class==1])
plt.legend(['has ASD','no ASD'])

plt.figure(figsize=(3,3))
sns.distplot(data1.A6_Score[data1.Class==0])
sns.distplot(data1.A6_Score[data1.Class==1])
plt.legend(['has ASD','no ASD'])

plt.figure(figsize=(3,3))
sns.distplot(data1.A7_Score[data1.Class==0])
sns.distplot(data1.A7_Score[data1.Class==1])
plt.legend(['has ASD','no ASD'])

plt.figure(figsize=(3,3))
sns.distplot(data1.A8_Score[data1.Class==0])
sns.distplot(data1.A8_Score[data1.Class==1])
plt.legend(['has ASD','no ASD'])

plt.figure(figsize=(3,3))
sns.distplot(data1.A9_Score[data1.Class==0])
sns.distplot(data1.A9_Score[data1.Class==1])
plt.legend(['has ASD','no ASD'])

plt.figure(figsize=(3,3))
sns.distplot(data1.A10_Score[data1.Class==0])
sns.distplot(data1.A10_Score[data1.Class==1])
plt.legend(['has ASD','no ASD'])

sns.countplot(x=df.gender,hue=data1.Class)
plt.show()

sns.countplot(x=df.jaundice,hue=data1.Class)
plt.show()

sns.countplot(x=df.ethnicity,hue=data1.Class)
plt.show()

from sklearn.preprocessing import LabelEncoder

le1=LabelEncoder()
df.Sex=le1.fit_transform(df.gender)

le1=LabelEncoder()
df.ethnicity=le1.fit_transform(df.ethnicity)

ip=data1.drop(['Class'],axis=1)
op=data1['Class']

import numpy as np
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
ct=ColumnTransformer([('gender',OneHotEncoder(),[1]),
                      ('ethnicity',OneHotEncoder(),[2])],
                     remainder='passthrough')
ip=np.array(ct.fit_transform(ip),dtype=str)

ip

from sklearn.model_selection import train_test_split
xtr,xts,ytr,yts=train_test_split(ip,op,test_size=0.2)

from sklearn.preprocessing import StandardScaler
sc=StandardScaler()
sc.fit(xtr)
sc.fit(xts)
xtr=sc.transform(xtr)
xts=sc.transform(xts)

from sklearn.neighbors import KNeighborsClassifier
alg=KNeighborsClassifier(n_neighbors=3)
alg.fit(xtr,ytr)
yp2=alg.predict(xts)

from sklearn import metrics
accuracy=metrics.accuracy_score(yts,yp2)
recall=metrics.recall_score(yts,yp2)
precision=metrics.precision_score(yts,yp2)
F1_score=metrics.f1_score(yts,yp2)
cf_matrix=metrics.confusion_matrix(yts,yp2)
print(F1_score)
print(precision)
print(recall)
print(accuracy)
print(cf_matrix)

from sklearn.naive_bayes import GaussianNB
clf=GaussianNB()
clf.fit(xtr,ytr)
yp1=clf.predict(xts)

from sklearn import metrics
accuracy=metrics.accuracy_score(yts,yp1)
recall=metrics.recall_score(yts,yp1)
precision=metrics.precision_score(yts,yp2)
F1_score=metrics.f1_score(yts,yp2)
cf_matrix=metrics.confusion_matrix(yts,yp1)
print(F1_score)
print(precision)
print(recall)
print(accuracy)
print(cf_matrix)

from sklearn.metrics import roc_curve, auc

KNN_fpr, KNN_tpr, threshold = roc_curve(yts, yp2)
auc_KNN = auc(KNN_fpr, KNN_tpr)

NB_fpr, NB_tpr, threshold = roc_curve(yts, yp1)
auc_NB = auc(NB_fpr, NB_tpr)

plt.figure(figsize=(4, 4), dpi=100)
plt.plot(NB_fpr, NB_tpr, linestyle='-', label='NB (auc = %0.3f)' % auc_NB)
plt.plot(KNN_fpr, KNN_tpr, marker='.', label='KNN (auc = %0.3f)' % auc_KNN)

plt.xlabel('False Positive Rate -->')
plt.ylabel('True Positive Rate -->')

plt.legend()

plt.show()

from sklearn.metrics import precision_recall_curve, average_precision_score
y_scores = clf.predict_proba(xts)[:, 1]

# Compute precision-recall curve
precision, recall, thresholds = precision_recall_curve(yts, y_scores)

# Compute average precision score
avg_precision = average_precision_score(yts, y_scores)

# Plot the precision-recall curve
plt.figure(figsize=(8, 6))
plt.plot(recall, precision, label=f'AP = {avg_precision:.2f}', color='blue', lw=2)
plt.xlabel('Recall', fontsize=14)
plt.ylabel('Precision', fontsize=14)
plt.title('Precision-Recall Curve', fontsize=16)
plt.legend(loc='best', fontsize=12)
plt.grid(alpha=0.3)
plt.show()